{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Included\n",
    "- SVM\n",
    "- Bi-LSTM\n",
    "- BiLSTM with Attention\n",
    "- LSTM CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, SimpleRNN, Flatten,\\\n",
    "Activation, RepeatVector, Permute, merge, Lambda\n",
    "from keras_contrib.layers import CRF\n",
    "import keras.optimizers as ko\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import keras\n",
    "import random\n",
    "import subprocess\n",
    "from sklearn.utils import class_weight\n",
    "import keras.backend as K\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('task2_typ.csv')\n",
    "data = data.rename(columns={'id':'Sentence #'})\n",
    "data = data.dropna(axis=0)\n",
    "data = data.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = list(data['type']) \n",
    "typs = data['types'].values\n",
    "x = data.apply(lambda l: sentences.append(l['sentence'].split(' ')),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sent in sentences:\n",
    "    for wrd in sent:\n",
    "        words.append(wrd)\n",
    "words = list(set(words))\n",
    "tags = list(set(labels))\n",
    "\n",
    "n_words = len(words)\n",
    "n_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(typs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "max_len = 30\n",
    "X = [[word2idx[w] for w in s] for s in sentences]    \n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
    "y = [tag2idx[tg] for tg in labels]\n",
    "# y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "y = np.array([to_categorical(i, num_classes=n_tags) for i in y])\n",
    "y = [[y[i],typs[i]] for i in range(len(y))]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "y_train = np.array([lab[0] for lab in y_train])\n",
    "typ_test = [lab[1] for lab in y_test]\n",
    "y_test = np.array([lab[0] for lab  in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(File,'r')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel\n",
    "\n",
    "vec_model = loadGloveModel('glove/glove.6B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = len(vec_model['the'])\n",
    "embedding_matrix = np.zeros((len(word2idx) + 1, emb_dim))\n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    if word not in vec_model:\n",
    "        continue\n",
    "    embedding_vector = vec_model[word]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_results(y_test,pred):\n",
    "    \n",
    "    report = classification_report(y_test,pred.flatten('F'),output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    display(df)\n",
    "    \n",
    "    inc_test = []\n",
    "    exc_test = []\n",
    "    inc_pred = []\n",
    "    exc_pred = []\n",
    "    for i,t in enumerate(typ_test):\n",
    "        if t == 'I':\n",
    "            inc_test.append(y_test[i])\n",
    "            inc_pred.append(pred[i])\n",
    "        else:\n",
    "            exc_test.append(y_test[i])\n",
    "            exc_pred.append(pred[i])\n",
    "    print(\"For inclusions\")\n",
    "    report = classification_report(inc_test,inc_pred,output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    display(df)\n",
    "    print(\"For exclusions\")\n",
    "    report = classification_report(exc_test,exc_pred,output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    display(df)\n",
    "    return\n",
    "\n",
    "def return_report(model,epochs):\n",
    "    y_flat = list(np.argmax(y_test,1).flatten('F'))\n",
    "#     class_weights = class_weight.compute_class_weight('balanced',y_flat)\n",
    "#     class_weights = [1,10,20,10,20]\n",
    "    model.fit(X_train,y_train,epochs=epochs,verbose=1)\n",
    "    out = model.predict(X_test)\n",
    "    pred = np.argmax(out,1)\n",
    "    full_results(y_flat,pred)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    \n",
    "    input = Input(shape=(max_len,))\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model = Embedding(input_dim=n_words + 1, output_dim=emb_dim,\n",
    "                  input_length=max_len, weights=[embedding_matrix],trainable=False)(input)\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model = Bidirectional(LSTM(units=emb_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.1))(model)    \n",
    "    \n",
    "    model = TimeDistributed(Dense(100,activation='relu'))(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(100,activation='relu')(model)\n",
    "    # Add timeDistributed Layer\n",
    "    out = Dense(n_tags, activation=\"softmax\")(model)\n",
    "\n",
    "    #Optimiser \n",
    "    adam = ko.Adam(lr=0.0007)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(input, out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def lstm_attention():\n",
    "    \n",
    "    input = Input(shape=(max_len,))\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model = Embedding(input_dim=n_words + 1, output_dim=emb_dim,\n",
    "                  input_length=max_len, weights=[embedding_matrix],trainable=False)(input)\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model = Bidirectional(LSTM(units=emb_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.1))(model)\n",
    "\n",
    "    attention = TimeDistributed(Dense(1, activation='tanh'))(model) \n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(2*emb_dim)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    # apply the attention\n",
    "    sent_representation = merge.multiply([model, attention])\n",
    "    sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
    "    probabilities = Dense(3, activation='softmax')(sent_representation)\n",
    "    \n",
    "    # Add timeDistributed Layer\n",
    "    out = Dense(n_tags, activation=\"softmax\")(sent_representation)\n",
    "\n",
    "    #Optimiser \n",
    "    adam = ko.Adam(lr=0.0007)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(input, out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def lstm_cnn():\n",
    "    input = Input(shape=(max_len,))\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model = Embedding(input_dim=n_words + 1, output_dim=emb_dim,\n",
    "                  input_length=max_len, weights=[embedding_matrix],trainable=False)(input)\n",
    "    model = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = LSTM(100,dropout=0.2,recurrent_dropout=0.2)(model)\n",
    "    out = Dense(n_tags, activation='softmax')(model)\n",
    "    model = Model(input,out)\n",
    "    adam = ko.Adam(lr=0.0007)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilistm = get_bilstm_lstm_model()\n",
    "pred = return_report(bilistm,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = lstm_attention()\n",
    "preds = return_report(attn,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmcnn = lstm_cnn()\n",
    "preds = return_report(lstmcnn,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilistm.save('task2_models/bi-lstm/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "max_len = 30\n",
    "emb_dim = len(vec_model['the'])\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sent in sentences:\n",
    "    vec = np.zeros((30,emb_dim))\n",
    "    for i, word in enumerate(sent):\n",
    "        if word in vec_model:\n",
    "            vec[i,:] = vec_model[word]\n",
    "    X.append(vec)\n",
    "X = np.array(X)\n",
    "y = np.array([tag2idx[tg] for tg in labels])\n",
    "y = [[y[i],typs[i]] for i in range(len(y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train = np.array([lab[0] for lab in y_train])\n",
    "typ_test = [lab[1] for lab in y_test]\n",
    "y_test = np.array([lab[0] for lab  in y_test])\n",
    "X_train = X_train.reshape((X_train.shape[0],max_len*emb_dim))\n",
    "X_test = X_test.reshape((X_test.shape[0],max_len*emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly')\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict(X_test)\n",
    "full_results(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
